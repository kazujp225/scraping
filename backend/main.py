"""
FastAPI Backend for Job Scraper
"""
import asyncio
import json
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import logging

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
import sys
import os

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from scraper_real import scrape_indeed_real, scrape_yahoo_real, scrape_townwork_real
from scraper_simple import scrape_indeed_demo, scrape_yahoo_demo, scrape_townwork_demo

# ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°ï¼ˆç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡å¯èƒ½ï¼‰
DEMO_MODE = os.getenv("DEMO_MODE", "false").lower() == "true"

logger.info("=" * 60)
logger.info("Job Scraper API Starting...")
logger.info(f"Mode: {'DEMO' if DEMO_MODE else 'REAL SCRAPING'}")
logger.info("=" * 60)

app = FastAPI(title="Job Scraper API", version="1.0.0")

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
class ScrapeConfig(BaseModel):
    site: str
    keyword: str
    location: str
    maxPages: int = 5

class ScrapeRequest(BaseModel):
    configs: List[ScrapeConfig]

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
sessions: Dict[str, Dict] = {}
websocket_connections: Dict[str, WebSocket] = {}

# ã‚µã‚¤ãƒˆæƒ…å ±
SITES = [
    {"id": "townwork", "name": "ã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯", "description": "åœ°åŸŸå¯†ç€å‹æ±‚äºº", "enabled": True, "icon": "ğŸª"},
]

@app.get("/api/sites")
async def get_sites():
    """ã‚µã‚¤ãƒˆä¸€è¦§ã‚’å–å¾—"""
    logger.info("ğŸ“‹ ã‚µã‚¤ãƒˆä¸€è¦§ã‚’å–å¾—")
    return SITES

@app.post("/api/scrape/start")
async def start_scraping(request: ScrapeRequest):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹"""
    session_id = str(uuid.uuid4())

    logger.info("=" * 60)
    logger.info(f"ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ - Session: {session_id}")
    logger.info(f"   ã‚µã‚¤ãƒˆæ•°: {len(request.configs)}")
    for config in request.configs:
        logger.info(f"   - {config.site}: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={config.keyword}, åœ°åŸŸ={config.location}, ãƒšãƒ¼ã‚¸æ•°={config.maxPages}")
    logger.info("=" * 60)

    sessions[session_id] = {
        "configs": [c.model_dump() for c in request.configs],
        "results": [],
        "status": "running",
        "startTime": datetime.now().isoformat(),
    }

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    asyncio.create_task(run_scraping(session_id, request.configs))

    return {"sessionId": session_id}

async def run_scraping(session_id: str, configs: List[ScrapeConfig]):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
    logger.info(f"ğŸ”„ Session {session_id}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†é–‹å§‹")
    try:
        results = []

        for idx, config in enumerate(configs, 1):
            logger.info(f"ğŸ“ [{idx}/{len(configs)}] {config.site} ã®å‡¦ç†é–‹å§‹...")
            # WebSocketçµŒç”±ã§é€²æ—ã‚’é€ä¿¡
            if session_id in websocket_connections:
                await websocket_connections[session_id].send_json({
                    "type": "progress",
                    "data": {
                        "site": config.site,
                        "status": "running",
                        "currentPage": 1,
                        "totalPages": config.maxPages,
                        "itemsCollected": 0,
                        "message": f"{config.site}ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹",
                    }
                })

            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            jobs = []
            start_time = datetime.now()

            try:
                logger.info(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {config.keyword}, åœ°åŸŸ: {config.location}")
                logger.info(f"   ãƒ¢ãƒ¼ãƒ‰: {'DEMO' if DEMO_MODE else 'REAL'}")

                if DEMO_MODE:
                    # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
                    logger.info(f"   ğŸ­ ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
                    if config.site == "townwork":
                        jobs = await scrape_townwork_demo(config.keyword, config.location)
                    else:
                        jobs = await scrape_townwork_demo(config.keyword, config.location)
                else:
                    # å®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    logger.info(f"   ğŸŒ å®Ÿéš›ã®ã‚µã‚¤ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
                    if config.site == "townwork":
                        jobs = await scrape_townwork_real(config.keyword, config.location, config.maxPages * 10)
                    else:
                        logger.warning(f"   âš ï¸  {config.site} ã¯æœªå¯¾å¿œã§ã™ï¼ˆã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯é™å®šãƒ¢ãƒ¼ãƒ‰ï¼‰")
                        jobs = []

                duration = (datetime.now() - start_time).total_seconds()

                result = {
                    "site": config.site,
                    "jobs": jobs,
                    "totalItems": len(jobs),
                    "duration": duration,
                    "timestamp": datetime.now().isoformat(),
                    "success": True,
                }

                results.append(result)

                logger.info(f"   âœ… {config.site} å®Œäº†: {len(jobs)}ä»¶å–å¾— ({duration:.2f}ç§’)")

                # å®Œäº†é€šçŸ¥
                if session_id in websocket_connections:
                    await websocket_connections[session_id].send_json({
                        "type": "progress",
                        "data": {
                            "site": config.site,
                            "status": "completed",
                            "currentPage": config.maxPages,
                            "totalPages": config.maxPages,
                            "itemsCollected": len(jobs),
                            "message": f"{len(jobs)}ä»¶å–å¾—å®Œäº†",
                        }
                    })

            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼é€šçŸ¥
                logger.error(f"   âŒ {config.site} ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
                if session_id in websocket_connections:
                    await websocket_connections[session_id].send_json({
                        "type": "progress",
                        "data": {
                            "site": config.site,
                            "status": "error",
                            "currentPage": 0,
                            "totalPages": config.maxPages,
                            "itemsCollected": 0,
                            "error": str(e),
                        }
                    })

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ›´æ–°
        sessions[session_id]["results"] = results
        sessions[session_id]["status"] = "completed"
        sessions[session_id]["endTime"] = datetime.now().isoformat()

        total_jobs = sum(r["totalItems"] for r in results)
        logger.info("=" * 60)
        logger.info(f"ğŸ‰ Session {session_id}: å…¨å‡¦ç†å®Œäº†")
        logger.info(f"   åˆè¨ˆå–å¾—ä»¶æ•°: {total_jobs}ä»¶")
        logger.info("=" * 60)

        # å®Œäº†é€šçŸ¥
        if session_id in websocket_connections:
            await websocket_connections[session_id].send_json({
                "type": "complete",
                "data": results,
            })

    except Exception as e:
        logger.error(f"âŒ Session {session_id}: è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        sessions[session_id]["status"] = "error"
        sessions[session_id]["error"] = str(e)

        if session_id in websocket_connections:
            await websocket_connections[session_id].send_json({
                "type": "error",
                "error": str(e),
            })

@app.get("/api/scrape/status/{session_id}")
async def get_scrape_status(session_id: str):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çŠ¶æ…‹å–å¾—"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="Session not found")

    return sessions[session_id].get("results", [])

@app.post("/api/scrape/stop/{session_id}")
async def stop_scraping(session_id: str):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åœæ­¢"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="Session not found")

    sessions[session_id]["status"] = "cancelled"
    return {"message": "Scraping stopped"}

@app.get("/api/export/{session_id}/{format}")
async def export_results(session_id: str, format: str):
    """çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="Session not found")

    results = sessions[session_id].get("results", [])

    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    all_jobs = []
    for result in results:
        all_jobs.extend(result["jobs"])

    if format == "json":
        # JSONå‡ºåŠ›
        output_path = Path(f"export_{session_id}.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_jobs, f, ensure_ascii=False, indent=2)

        return FileResponse(
            output_path,
            media_type="application/json",
            filename=f"æ±‚äººãƒ‡ãƒ¼ã‚¿_{datetime.now().strftime('%Y%m%d')}.json"
        )

    elif format == "excel":
        # Excelå‡ºåŠ›
        import pandas as pd

        df = pd.DataFrame(all_jobs)
        output_path = Path(f"export_{session_id}.xlsx")
        df.to_excel(output_path, index=False, engine="openpyxl")

        return FileResponse(
            output_path,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            filename=f"æ±‚äººãƒ‡ãƒ¼ã‚¿_{datetime.now().strftime('%Y%m%d')}.xlsx"
        )

    else:
        raise HTTPException(status_code=400, detail="Invalid format")

@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """WebSocketæ¥ç¶š"""
    await websocket.accept()
    websocket_connections[session_id] = websocket

    try:
        while True:
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¾…ã¤
            data = await websocket.receive_text()

    except WebSocketDisconnect:
        if session_id in websocket_connections:
            del websocket_connections[session_id]

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆ"""
    return {
        "message": "Job Scraper API",
        "version": "1.0.0",
        "mode": "DEMO" if DEMO_MODE else "REAL",
        "docs": "/docs"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
