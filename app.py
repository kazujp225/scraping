"""
æ±‚äººã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°GUIã‚·ã‚¹ãƒ†ãƒ 
Streamlit ãƒ™ãƒ¼ã‚¹
"""
import streamlit as st
import asyncio
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import sys
import logging

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯é™å®šï¼‰
from scrapers.townwork import TownworkScraper

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ±‚äººã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        margin-bottom: 1rem;
    }
    .stButton>button {
        width: 100%;
        background-color: #1f77b4;
        color: white;
    }
    .success-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
    }
    .error-box {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
    }
</style>
""", unsafe_allow_html=True)


def load_config():
    """ã‚»ãƒ¬ã‚¯ã‚¿è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    config_path = Path("config/selectors.json")
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_config(config):
    """ã‚»ãƒ¬ã‚¯ã‚¿è¨­å®šã‚’ä¿å­˜"""
    config_path = Path("config/selectors.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)


def get_scraper(site_name: str):
    """ã‚µã‚¤ãƒˆåã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    # ã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯ä»¥å¤–ã¯ç„¡åŠ¹åŒ–
    scrapers = {
        "townwork": TownworkScraper,
    }
    scraper_class = scrapers.get(site_name)
    if scraper_class:
        return scraper_class()
    return None


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown('<p class="main-header">ğŸ” æ±‚äººã‚µã‚¤ãƒˆ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ </p>', unsafe_allow_html=True)
    st.markdown("---")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")

        # ã‚¿ãƒ–é¸æŠ
        page = st.radio(
            "ãƒ¡ãƒ‹ãƒ¥ãƒ¼",
            ["ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ", "ğŸ› ï¸ ã‚µã‚¤ãƒˆç®¡ç†", "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç¢ºèª"],
            label_visibility="collapsed"
        )

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()

    # ===== ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œã‚¿ãƒ– =====
    if page == "ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ":
        st.header("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ")

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("å¯¾è±¡ã‚µã‚¤ãƒˆé¸æŠ")

            # ã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯ã®ã¿é¸æŠå¯èƒ½
            available_sites = {
                "townwork": "ã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯",
            }

            selected_sites = []
            for site_key, site_name in available_sites.items():
                # ã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ
                if st.checkbox(site_name, key=f"site_{site_key}", value=True):
                    selected_sites.append(site_key)

        with col2:
            st.subheader("æ¤œç´¢æ¡ä»¶")

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
            keywords_input = st.text_area(
                "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ1è¡Œ1ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰",
                value="IT\nå–¶æ¥­\né£²é£Ÿ",
                height=100
            )
            keywords = [k.strip() for k in keywords_input.split('\n') if k.strip()]

            # åœ°åŸŸå…¥åŠ›
            areas_input = st.text_area(
                "åœ°åŸŸï¼ˆ1è¡Œ1åœ°åŸŸï¼‰",
                value="æ±äº¬\nå¤§é˜ª",
                height=80
            )
            areas = [a.strip() for a in areas_input.split('\n') if a.strip()]

            # ãƒšãƒ¼ã‚¸æ•°
            max_pages = st.slider("æœ€å¤§ãƒšãƒ¼ã‚¸æ•°", min_value=1, max_value=20, value=5)

            # ä¸¦åˆ—æ•°
            parallel = st.select_slider(
                "ä¸¦åˆ—å®Ÿè¡Œæ•°",
                options=[1, 5, 10, 20, 50],
                value=10
            )

        # çµã‚Šè¾¼ã¿
        with st.expander("ğŸ” çµã‚Šè¾¼ã¿ï¼ˆã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯ï¼‰", expanded=False):
            colf1, colf2, colf3 = st.columns(3)

            with colf1:
                employment_types = st.multiselect(
                    "é›‡ç”¨å½¢æ…‹",
                    options=["ã‚¢ãƒ«ãƒã‚¤ãƒˆ", "ãƒ‘ãƒ¼ãƒˆ", "æ­£ç¤¾å“¡"],
                    default=["ã‚¢ãƒ«ãƒã‚¤ãƒˆ", "ãƒ‘ãƒ¼ãƒˆ", "æ­£ç¤¾å“¡"],
                )

            with colf2:
                salary_min = st.number_input(
                    "æœ€ä½çµ¦ä¸ï¼ˆå††/æ™‚ï¼‰",
                    min_value=0,
                    max_value=10000,
                    value=0,
                    step=50,
                )

            with colf3:
                shifts = st.multiselect(
                    "ã‚·ãƒ•ãƒˆ",
                    options=["æ—¥å‹¤", "å¤œå‹¤"],
                    default=[],
                )

            # ãƒ•ã‚£ãƒ«ã‚¿è¾æ›¸ã‚’çµ„ã¿ç«‹ã¦
            filters = {}
            if employment_types:
                filters["employment_type"] = employment_types
            if salary_min and salary_min > 0:
                filters["salary_min"] = int(salary_min)
            if shifts:
                filters["shift"] = shifts

        st.markdown("---")

        # å®Ÿè¡Œè¨­å®šã‚µãƒãƒªãƒ¼
        st.subheader("ğŸ“‹ å®Ÿè¡Œã‚µãƒãƒªãƒ¼")
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("å¯¾è±¡ã‚µã‚¤ãƒˆæ•°", len(selected_sites))
        with col2:
            st.metric("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°", len(keywords))
        with col3:
            st.metric("åœ°åŸŸæ•°", len(areas))
        with col4:
            total_tasks = len(selected_sites) * len(keywords) * len(areas)
            st.metric("ç·ã‚¿ã‚¹ã‚¯æ•°", total_tasks)

        st.markdown("---")

        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        if st.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary", use_container_width=True):
            if not selected_sites:
                st.error("âŒ å°‘ãªãã¨ã‚‚1ã¤ã®ã‚µã‚¤ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„")
            elif not keywords:
                st.error("âŒ æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            elif not areas:
                st.error("âŒ åœ°åŸŸã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                run_scraping(selected_sites, keywords, areas, max_pages, parallel, filters)

    # ===== ã‚µã‚¤ãƒˆç®¡ç†ã‚¿ãƒ– =====
    elif page == "ğŸ› ï¸ ã‚µã‚¤ãƒˆç®¡ç†":
        st.header("ğŸ› ï¸ ã‚µã‚¤ãƒˆç®¡ç†")

        st.info("ğŸ’¡ å„ã‚µã‚¤ãƒˆã®ã‚»ãƒ¬ã‚¯ã‚¿è¨­å®šã‚’ç®¡ç†ã—ã¾ã™ã€‚ã‚µã‚¤ãƒˆã®HTMLæ§‹é€ ãŒå¤‰ã‚ã£ãŸå ´åˆã€ã“ã“ã§æ›´æ–°ã§ãã¾ã™ã€‚")

        # ã‚µã‚¤ãƒˆé¸æŠ
        # ç®¡ç†å¯¾è±¡ã‚µã‚¤ãƒˆã‚’ã‚¿ã‚¦ãƒ³ãƒ¯ãƒ¼ã‚¯ã«é™å®š
        site_keys = [k for k in config.keys() if k == "townwork"]
        if site_keys:
            selected_site = st.selectbox("ã‚µã‚¤ãƒˆé¸æŠ", site_keys)

            if selected_site:
                st.subheader(f"ğŸ“ {config[selected_site].get('name', selected_site)} ã®è¨­å®š")

                # åŸºæœ¬æƒ…å ±
                with st.expander("åŸºæœ¬æƒ…å ±", expanded=True):
                    site_name = st.text_input("ã‚µã‚¤ãƒˆå", value=config[selected_site].get('name', ''))
                    base_url = st.text_input("ãƒ™ãƒ¼ã‚¹URL", value=config[selected_site].get('base_url', ''))
                    search_url = st.text_area(
                        "æ¤œç´¢URLãƒ‘ã‚¿ãƒ¼ãƒ³",
                        value=config[selected_site].get('search_url_pattern', ''),
                        height=80
                    )

                # ã‚»ãƒ¬ã‚¯ã‚¿è¨­å®š
                with st.expander("ã‚»ãƒ¬ã‚¯ã‚¿è¨­å®š", expanded=True):
                    st.markdown("#### ä¸€è¦§ãƒšãƒ¼ã‚¸ç”¨ã‚»ãƒ¬ã‚¯ã‚¿")
                    selectors = config[selected_site].get('selectors', {})

                    col1, col2 = st.columns(2)

                    with col1:
                        selectors['job_cards'] = st.text_input(
                            "æ±‚äººã‚«ãƒ¼ãƒ‰",
                            value=selectors.get('job_cards', '')
                        )
                        selectors['title'] = st.text_input(
                            "ã‚¿ã‚¤ãƒˆãƒ«",
                            value=selectors.get('title', '')
                        )
                        selectors['company'] = st.text_input(
                            "ä¼šç¤¾å",
                            value=selectors.get('company', '')
                        )
                        selectors['location'] = st.text_input(
                            "å‹¤å‹™åœ°",
                            value=selectors.get('location', '')
                        )

                    with col2:
                        selectors['salary'] = st.text_input(
                            "çµ¦ä¸",
                            value=selectors.get('salary', '')
                        )
                        selectors['employment_type'] = st.text_input(
                            "é›‡ç”¨å½¢æ…‹",
                            value=selectors.get('employment_type', '')
                        )
                        selectors['detail_link'] = st.text_input(
                            "è©³ç´°ãƒªãƒ³ã‚¯",
                            value=selectors.get('detail_link', '')
                        )

                # ä¿å­˜ãƒœã‚¿ãƒ³
                if st.button("ğŸ’¾ è¨­å®šã‚’ä¿å­˜", type="primary"):
                    config[selected_site]['name'] = site_name
                    config[selected_site]['base_url'] = base_url
                    config[selected_site]['search_url_pattern'] = search_url
                    config[selected_site]['selectors'] = selectors

                    save_config(config)
                    st.success("âœ… è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ")

                # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½
                st.markdown("---")
                st.subheader("ğŸ§ª ã‚»ãƒ¬ã‚¯ã‚¿ãƒ†ã‚¹ãƒˆ")
                test_url = st.text_input("ãƒ†ã‚¹ãƒˆURL")
                if st.button("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ") and test_url:
                    st.info("ãƒ†ã‚¹ãƒˆæ©Ÿèƒ½ã¯å®Ÿè£…ä¸­ã§ã™...")

    # ===== ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚¿ãƒ– =====
    elif page == "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç¢ºèª":
        st.header("ğŸ“Š å–å¾—ãƒ‡ãƒ¼ã‚¿ç¢ºèª")

        # ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ä¸€è¦§
        output_dir = Path("data/output")
        if output_dir.exists():
            csv_files = list(output_dir.glob("*.csv"))
            excel_files = list(output_dir.glob("*.xlsx"))

            all_files = csv_files + excel_files

            if all_files:
                st.subheader("ğŸ“ ä¿å­˜æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«")

                file_names = [f.name for f in all_files]
                selected_file = st.selectbox("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ", file_names)

                if selected_file:
                    file_path = output_dir / selected_file

                    try:
                        if selected_file.endswith('.csv'):
                            df = pd.read_csv(file_path)
                        else:
                            df = pd.read_excel(file_path)

                        st.success(f"âœ… èª­ã¿è¾¼ã¿æˆåŠŸ: {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿")

                        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                        st.dataframe(df, use_container_width=True)

                        # çµ±è¨ˆæƒ…å ±
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(df))
                        with col2:
                            if 'site' in df.columns:
                                st.metric("ã‚µã‚¤ãƒˆæ•°", df['site'].nunique())
                        with col3:
                            if 'company' in df.columns:
                                st.metric("ä¼æ¥­æ•°", df['company'].nunique())

                    except Exception as e:
                        st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.info("ğŸ’¡ ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        else:
            st.info("ğŸ’¡ å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")


def run_scraping(selected_sites, keywords, areas, max_pages, parallel, filters):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""

    st.subheader("ğŸ”„ å®Ÿè¡Œä¸­...")

    # é€²æ—ãƒãƒ¼
    progress_bar = st.progress(0)
    status_text = st.empty()
    log_container = st.container()

    all_results = []
    total_sites = len(selected_sites)

    try:
        for idx, site_key in enumerate(selected_sites):
            status_text.text(f"ğŸ“¡ {site_key} ã‚’å‡¦ç†ä¸­... ({idx + 1}/{total_sites})")

            scraper = get_scraper(site_key)
            if not scraper:
                with log_container:
                    st.warning(f"âš ï¸ {site_key} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue

            # éåŒæœŸå®Ÿè¡Œ
            with log_container:
                st.info(f"ğŸš€ {site_key} ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")

            try:
                results = asyncio.run(
                    scraper.scrape(keywords, areas, max_pages, parallel, filters=filters)
                )

                all_results.extend(results)

                with log_container:
                    st.success(f"âœ… {site_key} å®Œäº†: {len(results)} ä»¶å–å¾—")

            except Exception as e:
                with log_container:
                    st.error(f"âŒ {site_key} ã‚¨ãƒ©ãƒ¼: {str(e)}")
                logger.error(f"Scraping error for {site_key}: {e}", exc_info=True)

            # é€²æ—æ›´æ–°
            progress_bar.progress((idx + 1) / total_sites)

        # çµæœã‚’DataFrameã«å¤‰æ›
        if all_results:
            df = pd.DataFrame(all_results)

            st.markdown("---")
            st.subheader("ğŸ“Š å–å¾—çµæœ")

            # çµ±è¨ˆæƒ…å ±
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç·å–å¾—ä»¶æ•°", len(df))
            with col2:
                st.metric("ã‚µã‚¤ãƒˆæ•°", df['site'].nunique() if 'site' in df.columns else 0)
            with col3:
                st.metric("ä¼æ¥­æ•°", df['company'].nunique() if 'company' in df.columns else 0)

            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            st.dataframe(df.head(50), use_container_width=True)

            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
            st.subheader("ğŸ’¾ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
            col1, col2 = st.columns(2)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            with col1:
                csv_data = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="ğŸ“¥ CSV ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv_data,
                    file_name=f"scraping_results_{timestamp}.csv",
                    mime="text/csv"
                )

            with col2:
                # Excelä¿å­˜
                output_path = Path("data/output") / f"scraping_results_{timestamp}.xlsx"
                output_path.parent.mkdir(parents=True, exist_ok=True)
                df.to_excel(output_path, index=False, engine='openpyxl')
                st.success(f"âœ… Excelä¿å­˜: {output_path.name}")

            st.success("ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†ï¼")

        else:
            st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    except Exception as e:
        st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        logger.error(f"Scraping execution error: {e}", exc_info=True)
    finally:
        progress_bar.progress(100)
        status_text.text("âœ… å®Œäº†")


if __name__ == "__main__":
    main()
