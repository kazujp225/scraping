"""
タウンワーク専用スクレイパー
2024年更新版 - 新しいサイト構造に対応
"""
from typing import Dict, Any, List, Optional
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
from playwright.async_api import Page, Browser, TimeoutError as PlaywrightTimeoutError
from .base_scraper import BaseScraper
import logging
import re

logger = logging.getLogger(__name__)


class TownworkScraper(BaseScraper):
    """タウンワーク用スクレイパー"""

    def __init__(self):
        super().__init__(site_name="townwork")

    async def extract_job_card(self, card_element, page: Page) -> Dict[str, Any]:
        """
        タウンワーク用の求人カード情報抽出
        新しいCSS Module形式のクラス名に対応
        """
        job_data = {
            "site": "タウンワーク",
            "title": "",
            "company": "",
            "location": "",
            "salary": "",
            "employment_type": "",
            "url": "",
        }

        try:
            # 詳細ページへのリンク（カード自体がリンクの場合）
            href = await card_element.get_attribute("href")
            if href:
                if href.startswith("/"):
                    href = f"https://townwork.net{href}"
                job_data["url"] = href

                # 求人IDを抽出
                match = re.search(r"jobid_([a-f0-9]+)", href)
                if match:
                    job_data["job_id"] = match.group(1)

            # タイトル
            title_elem = await card_element.query_selector("[class*='title__']")
            if title_elem:
                job_data["title"] = (await title_elem.inner_text()).strip()

            # 会社名
            company_elem = await card_element.query_selector("[class*='employerName']")
            if company_elem:
                job_data["company"] = (await company_elem.inner_text()).strip()

            # 給与
            salary_elem = await card_element.query_selector("[class*='salaryText']")
            if salary_elem:
                job_data["salary"] = (await salary_elem.inner_text()).strip()

            # アクセス・勤務地
            access_elem = await card_element.query_selector("[class*='accessText']")
            if access_elem:
                access_text = (await access_elem.inner_text()).strip()
                # "交通・アクセス " プレフィックスを除去
                job_data["location"] = re.sub(r"^交通・アクセス\s*", "", access_text)

            # 雇用形態
            job_type_elem = await card_element.query_selector("[class*='jobType']")
            if job_type_elem:
                job_data["employment_type"] = (await job_type_elem.inner_text()).strip()

        except Exception as e:
            logger.error(f"Error extracting job card: {e}")

        return job_data

    def generate_search_url(self, keyword: str, area: str, page: int = 1) -> str:
        """
        タウンワーク用の検索URL生成
        新しいURL形式: https://townwork.net/prefectures/{area}/job_search/?keyword={keyword}&page={page}
        """
        # エリア名をローマ字に変換
        area_codes = self.site_config.get("area_codes", {})
        area_code = area_codes.get(area, area.lower())

        url_pattern = self.site_config.get("search_url_pattern")
        base_url = url_pattern.format(area=area_code, keyword=keyword, page=page)

        # 新着順パラメータを付与（デフォルト: sort=1）
        sort_conf = self.site_config.get("sort", {})
        sort_param = sort_conf.get("param")
        sort_newest = sort_conf.get("newest")

        if sort_param and sort_newest is not None:
            from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse

            parsed = urlparse(base_url)
            query = dict(parse_qsl(parsed.query))
            query[sort_param] = sort_newest
            new_query = urlencode(query)
            base_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))

        return base_url

    async def search_jobs(self, page: Page, keyword: str, area: str, max_pages: int = 5) -> List[Dict[str, Any]]:
        """
        求人検索を実行し、結果を返す
        """
        all_jobs = []

        for page_num in range(1, max_pages + 1):
            url = self.generate_search_url(keyword, area, page_num)
            logger.info(f"Fetching page {page_num}: {url}")

            success = False
            # ページ取得・抽出をリトライ（最大2回）し、取りこぼしを減らす
            for attempt in range(2):
                try:
                    response = await page.goto(
                        url,
                        wait_until="networkidle",
                        timeout=30000 if attempt == 0 else 40000  # 2回目は少し長めに待つ
                    )

                    if response and response.status == 404:
                        logger.warning(f"Page not found: {url}")
                        break

                    card_selector = self.selectors.get("job_cards", "[class*='jobCard']")

                    # カードが描画されるまで数回リトライし、描画遅延による取りこぼしを減らす
                    selector_ready = False
                    for sel_attempt in range(4):
                        try:
                            await page.wait_for_selector(card_selector, timeout=2000 + 500 * sel_attempt)
                            selector_ready = True
                            break
                        except PlaywrightTimeoutError:
                            logger.warning(
                                f"Job cards selector timeout on page {page_num} (attempt {sel_attempt + 1}/4). Retrying after short wait."
                            )
                            await page.wait_for_timeout(600 + 200 * sel_attempt)

                    if not selector_ready:
                        logger.warning(
                            f"Job cards selector not ready on page {page_num}; attempt {attempt + 1}/2. Retrying page if attempts remain."
                        )
                        if attempt == 0:
                            continue  # もう一度このページをやり直す

                    # 求人カードを取得
                    job_cards = await page.query_selector_all(card_selector)

                    # 0件の場合は短い待機のあと再取得（描画遅延対策）
                    if len(job_cards) == 0:
                        await page.wait_for_timeout(1000)
                        job_cards = await page.query_selector_all(card_selector)

                    # それでも0件なら別のリトライ機会があればやり直す
                    if len(job_cards) == 0:
                        logger.warning(f"No job cards found on page {page_num} (attempt {attempt + 1}/2).")
                        if attempt == 0:
                            await page.wait_for_timeout(1200)
                            continue
                        else:
                            logger.info(f"No jobs on page {page_num} after retries; stopping.")
                            success = True  # これ以上ないので終了扱い
                            break

                    logger.info(f"Found {len(job_cards)} jobs on page {page_num} (attempt {attempt + 1})")

                    for card in job_cards:
                        try:
                            job_data = await self._extract_card_data(card)
                            if job_data:
                                all_jobs.append(job_data)
                        except Exception as e:
                            logger.error(f"Error extracting job card: {e}")
                            continue

                    success = True
                    break  # ページ処理成功

                except Exception as e:
                    logger.error(f"Error fetching page {page_num} (attempt {attempt + 1}/2): {e}")
                    if attempt == 0:
                        await page.wait_for_timeout(1500)
                        continue
                    else:
                        break

            if not success:
                break

            # 次のページがあるか確認（見えなくても最大ページ数までは試行し、取りこぼしを減らす）
            next_page = await page.query_selector(f"[class*='pageButton']:has-text('{page_num + 1}')")
            if not next_page and page_num < max_pages:
                logger.warning(
                    f"Next page button not found for page {page_num}, but continuing to page {page_num + 1} to avoid missing results."
                )
                continue
            if not next_page:
                logger.info("No more pages available")
                break

        return all_jobs

    async def _extract_card_data(self, card) -> Optional[Dict[str, Any]]:
        """
        求人カードからデータを抽出
        """
        try:
            data = {}

            # 詳細ページへのリンク（カード自体 or 内部のanchor）
            href = await card.get_attribute("href")
            if not href:
                link_elem = await card.query_selector("a[href*='jobid_'], a[href^='/jobid_'], a[href*='job/'], a[href]")
                if link_elem:
                    href = await link_elem.get_attribute("href")
            if href:
                if href.startswith("/"):
                    href = f"https://townwork.net{href}"
                # クエリやフラグメントで差分が出ないよう正規化
                href = self._normalize_url(href)
                data["page_url"] = href

                # 求人IDを抽出
                match = re.search(r"jobid_([a-f0-9]+)", href)
                if match:
                    data["job_number"] = match.group(1)

            # タイトル
            title_elem = await card.query_selector("[class*='title__']")
            if title_elem:
                data["title"] = (await title_elem.inner_text()).strip()

            # 会社名
            company_elem = await card.query_selector("[class*='employerName']")
            if company_elem:
                data["company_name"] = (await company_elem.inner_text()).strip()

            # 給与
            salary_elem = await card.query_selector("[class*='salaryText']")
            if salary_elem:
                data["salary"] = (await salary_elem.inner_text()).strip()

            # アクセス・勤務地
            access_elem = await card.query_selector("[class*='accessText']")
            if access_elem:
                access_text = (await access_elem.inner_text()).strip()
                # "交通・アクセス " プレフィックスを除去
                data["location"] = re.sub(r"^交通・アクセス\s*", "", access_text)

            # 雇用形態
            job_type_elem = await card.query_selector("[class*='jobType']")
            if job_type_elem:
                data["employment_type"] = (await job_type_elem.inner_text()).strip()

            return data if data.get("page_url") else None

        except Exception as e:
            logger.error(f"Error extracting card data: {e}")
            return None

    def _normalize_url(self, url: str) -> str:
        """クエリ・フラグメントを除去して末尾スラッシュを揃える"""
        if not url:
            return ""
        from urllib.parse import urlparse, urlunparse

        parsed = urlparse(url)
        path = parsed.path or "/"
        path = path.rstrip("/") or "/"
        return urlunparse((parsed.scheme, parsed.netloc, path, "", "", ""))

    async def extract_detail_info(self, page: Page, url: str) -> Dict[str, Any]:
        """
        詳細ページから追加情報を取得

        取得項目:
        - 会社名カナ
        - 郵便番号
        - 住所詳細
        - 電話番号
        - FAX番号
        - 担当者名
        - 求人番号
        - 事業内容
        - 従業員数
        """
        detail_data = {}

        try:
            await page.goto(url, wait_until="networkidle", timeout=30000)
            await page.wait_for_timeout(2000)

            # ページ全体のテキストを取得して解析
            body_text = await page.inner_text("body")

            # 郵便番号と住所の抽出
            # パターン: 郵便番号 + 都道府県から始まる住所
            postal_match = re.search(r"(\d{3})-?(\d{4})(東京都|大阪府|北海道|京都府|.{2,3}県)(.+?)(?=\n|交通|地図|※)", body_text)
            if postal_match:
                detail_data["postal_code"] = postal_match.group(1) + postal_match.group(2)
                detail_data["address"] = postal_match.group(3) + postal_match.group(4).strip()
            else:
                # 別のパターン：郵便番号なしで都道府県から始まる
                addr_match = re.search(r"(東京都|大阪府|北海道|京都府|.{2,3}県)(.{5,50}?)(?=\n|交通|地図|※)", body_text)
                if addr_match:
                    detail_data["address"] = addr_match.group(1) + addr_match.group(2).strip()

            # 電話番号の抽出（代表電話番号）
            phone_match = re.search(r"代表電話番号\s*[\n\r]*(\d{10,11})", body_text)
            if phone_match:
                detail_data["phone"] = phone_match.group(1)
            else:
                # 別のパターン
                phone_match2 = re.search(r"電話番号[：:\s]*(\d{2,4}[-]?\d{2,4}[-]?\d{3,4})", body_text)
                if phone_match2:
                    detail_data["phone"] = phone_match2.group(1).replace("-", "")

            # 会社名
            company_elem = await page.query_selector("[class*='companyName'], [class*='employerName']")
            if company_elem:
                detail_data["company_name"] = (await company_elem.inner_text()).strip()

            # 事業内容
            business_match = re.search(r"事業内容\s*[\n\r]*(.+?)(?=\n所在|$)", body_text)
            if business_match:
                detail_data["business_content"] = business_match.group(1).strip()

            # 原稿ID（求人番号）
            job_id_match = re.search(r"原稿ID[：:\s]*([a-f0-9]+)", body_text)
            if job_id_match:
                detail_data["job_number"] = job_id_match.group(1)

            # 仕事内容
            desc_match = re.search(r"仕事内容\s*[\n\r]*(.+?)(?=\n勤務地|$)", body_text, re.DOTALL)
            if desc_match:
                detail_data["job_description"] = desc_match.group(1).strip()[:500]  # 最大500文字

            # 勤務時間
            time_match = re.search(r"勤務時間詳細\s*[\n\r]*勤務時間\s*[\n\r]*(.+?)(?=\n|$)", body_text)
            if time_match:
                detail_data["working_hours"] = time_match.group(1).strip()

            # 休日休暇
            holiday_match = re.search(r"休日休暇\s*[\n\r]*(.+?)(?=\n職場|$)", body_text)
            if holiday_match:
                detail_data["holidays"] = holiday_match.group(1).strip()

            # 応募資格
            qualification_match = re.search(r"求めている人材\s*[\n\r]*(.+?)(?=\n試用|$)", body_text, re.DOTALL)
            if qualification_match:
                detail_data["qualifications"] = qualification_match.group(1).strip()[:300]

        except Exception as e:
            logger.error(f"Error extracting detail info from {url}: {e}")

        return detail_data

    async def scrape_with_details(self, page: Page, keyword: str, area: str,
                                   max_pages: int = 5, fetch_details: bool = True) -> List[Dict[str, Any]]:
        """
        求人検索と詳細情報取得を実行
        """
        # まず検索結果を取得
        jobs = await self.search_jobs(page, keyword, area, max_pages)

        if not fetch_details:
            return jobs

        # 各求人の詳細情報を取得
        for i, job in enumerate(jobs):
            if job.get("page_url"):
                logger.info(f"Fetching detail {i+1}/{len(jobs)}: {job['page_url']}")
                try:
                    detail_data = await self.extract_detail_info(page, job["page_url"])
                    job.update(detail_data)
                    await page.wait_for_timeout(1000)  # サーバーに負荷をかけないよう待機
                except Exception as e:
                    logger.error(f"Error fetching detail for job {i+1}: {e}")

        return jobs
