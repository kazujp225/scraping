# 求人サイトスクレイピングシステム 要件定義書

## 1. プロジェクト概要

### 1.1 目的
複数の求人サイトから効率的にデータを抽出し、リスト化するシステムの開発

### 1.2 背景
- 現状のシステムでは3,000件のデータ取得に1時間程度かかる
- データ量によっては丸1日かかるケースもあり、業務効率が低下している

### 1.3 目標
- **抽出速度**: 現行の100倍以上の高速化（3,000件を数分で取得）
- **自動化**: 条件設定による自動ダウンロード機能の実装

---

## 2. 対象サイト（全11サイト）

1. Indeed（インディード）
2. ハローワーク
3. タウンワーク
4. バイトル
5. マッハバイト
6. LINEバイト
7. リクナビ
8. マイナビ
9. エン転職
10. カイゴジョブ
11. ジョブメドレー

---

## 3. 必須データ項目（取得フィールド）

| No. | 項目名 | 優先度 |
|-----|--------|--------|
| 1 | 会社名 | 必須 |
| 2 | カナ | 必須 |
| 3 | 郵便番号 | 必須 |
| 4 | 住所１ | 必須 |
| 5 | 電話番号 | 必須 |
| 6 | 求人番号 | 必須 |
| 7 | 職種 | 必須 |
| 8 | 担当者 | 任意 |
| 9 | 担当者メールアドレス | 任意 |
| 10 | ページURL | 必須 |
| 11 | FAX番号 | 任意 |
| 12 | 雇用形態 | 必須 |
| 13 | 採用人数 | 任意 |
| 14 | 事業内容 | 必須 |
| 15 | 就業場所 | 必須 |
| 16 | 求人タイトル | 必須 |
| 17 | 給与 | 必須 |
| 18 | 仕事内容 | 必須 |

**注**: サイトによって取得可能な項目が異なる場合は、取得可能な項目のみを抽出

---

## 4. 機能要件

### 4.1 高速スクレイピング機能
- **要件**: 現行システムの100倍以上の速度向上
- **実装方針**:
  - 非同期並列処理の採用
  - 直接URL生成によるページ操作の省略
  - 複数ブラウザコンテキストの活用（10-20並列）

### 4.2 検索条件設定機能
ユーザーが以下の条件を設定可能:
- **地域**: 複数選択可（例: 東京、大阪、愛知）
- **業種/職種**: 複数選択可（例: IT、営業、飲食）
- **取得ページ数**: 各条件あたりの最大ページ数

### 4.3 自動ダウンロード機能
- リストが更新され次第、自動的にデータを取得
- 設定した条件に基づいて定期実行
- 結果をCSVまたはExcel形式で保存

---

## 5. 技術アーキテクチャ

### 5.1 基本設計方針

#### Phase 1: セレクタマッピング
Playwright Codegenを使用して各サイトのHTML要素セレクタを抽出

**セレクタマッピングの例**:
```json
{
  "indeed": {
    "job_card": ".job_seen_beacon",
    "title": ".jobTitle span",
    "company": ".companyName",
    "location": ".companyLocation",
    "salary": ".salary-snippet",
    "description": ".job-snippet"
  },
  "townwork": {
    "job_card": ".c-list-work__body",
    "title": ".c-list-work__title",
    "company": ".c-list-work__company"
  }
}
```

#### Phase 2: 高速スクレイパー実装

**コア機能**:

1. **URL生成エンジン**
   ```python
   def generate_urls(site, area, category, max_pages=5):
       """
       検索条件からURLを自動生成
       例: site='indeed', area='東京', category='IT'
       → 複数ページのURL配列を返す
       """
   ```

2. **非同期並列取得**
   ```python
   async def scrape_all_sites(conditions):
       """
       11サイト × 複数条件を並列実行
       例: 11サイト × 3地域 × 3職種 = 99パターンを並列処理
       """
   ```

3. **最適化戦略**
   - 一覧ページから取得可能なデータは詳細ページにアクセスしない
   - 電話番号など一覧にないデータのみ詳細ページを取得

---

## 6. 性能目標

| 条件 | 取得件数 | 目標時間 | 備考 |
|------|---------|---------|------|
| 1サイト × 1条件 × 5ページ | 50件 | 3秒 | 単一条件テスト |
| 1サイト × 10条件 × 5ページ | 500件 | 15秒 | 複数条件テスト |
| 11サイト × 10条件 × 5ページ | 5,500件 | 3分 | フル稼働時 |

**現行との比較**:
- 現行: 3,000件 = 60分
- 目標: 3,000件 = 2-3分
- **速度向上率**: 約100倍

---

## 7. コマンドライン実行仕様

```bash
python scraper.py \
  --sites all \
  --areas 東京,大阪,愛知 \
  --categories IT,営業,飲食 \
  --pages 5 \
  --parallel 20 \
  --output data.csv

# 実行結果:
# - 11サイト × 3地域 × 3職種 × 5ページ = 495ページ取得
# - 20並列処理
# - 予想所要時間: 2-3分
```

---

## 8. 開発フェーズ

### Phase 1: プロトタイプ開発（3サイト）
- 対象: Indeed、タウンワーク、バイトル
- 基本機能の実装と性能検証
- 所要時間: 1-2週間

### Phase 2: 全サイト対応
- 残り8サイトのセレクタマッピング追加
- 各サイト固有の処理実装
- 所要時間: 2-3週間

### Phase 3: 自動化機能実装
- スケジューラー機能追加
- エラーハンドリング強化
- 所要時間: 1週間

---

## 9. 技術スタック

- **言語**: Python 3.9+
- **スクレイピング**: Playwright (非同期対応)
- **並列処理**: asyncio
- **データ出力**: CSV / Excel (pandas)
- **スケジューリング**: 要検討（cron / APScheduler など）

---

## 10. 注意事項

### 10.1 法的・倫理的配慮
- 各サイトの利用規約を遵守すること
- robots.txtの確認
- アクセス頻度の制限（Rate Limiting）
- User-Agentの適切な設定

### 10.2 技術的リスク
- サイトのHTML構造変更によるセレクタの無効化
- アクセス制限（IP BAN、CAPTCHA）
- サーバー負荷による一時的なアクセス不可

### 10.3 運用上の考慮事項
- 定期的なセレクタのメンテナンス
- エラー時の通知機能
- ログ記録とモニタリング

---

## 11. 成果物

1. **スクレイピングシステム本体**
   - セレクタマッピングJSON
   - スクレイパーモジュール（11サイト対応）
   - URL生成エンジン
   - 並列実行エンジン

2. **ドキュメント**
   - 使用方法マニュアル
   - セレクタ更新手順書
   - トラブルシューティングガイド

3. **実行環境**
   - requirements.txt
   - 環境構築手順書
   - Docker対応（オプション）

---

## 12. 次のアクション

1. プロトタイプ開発（3サイト）の着手
2. 性能測定とボトルネック分析
3. 必要に応じたアーキテクチャ調整
4. 残り8サイトへの展開
